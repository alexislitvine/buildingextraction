{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4821d07f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29cb3d7",
   "metadata": {},
   "source": [
    "### Run the following cells before navigating through the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57545422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SM_FRAMEWORK=tf.keras\n"
     ]
    }
   ],
   "source": [
    "%env SM_FRAMEWORK=tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56321c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional Code to reset GPU\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "print(device)\n",
    "#device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d411f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import warnings\n",
    "\n",
    "def function_that_warns():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    function_that_warns()  # this will not show a warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf94380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 13:08:54.138158: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import glob\n",
    "import cv2\n",
    "from patchify import patchify, unpatchify\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_classification_path(file_path, base_preds):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_name_no_ext = os.path.splitext(file_name)[0]\n",
    "    return os.path.join(base_preds, file_name_no_ext)\n",
    "\n",
    "def get_color_classification_path(file_path, color_ext):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_name_no_ext = os.path.splitext(file_name)[0]\n",
    "    return os.path.join(color_ext, file_name_no_ext)\n",
    "\n",
    "def classification_files_exist(cnnpred_file, colorpred_file):\n",
    "    return os.path.exists(cnnpred_file) and os.path.exists(colorpred_file)\n",
    "\n",
    "def get_fusion_path(file_path, fusion_ext):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_name_no_ext = os.path.splitext(file_name)[0]\n",
    "    return os.path.join(fusion_ext, file_name_no_ext)\n",
    "\n",
    "def process_images(cnnpred, colorpred):\n",
    "    \"\"\"Replace connected components in base predictions with connected components from color predictions.\"\"\"\n",
    "    # Find connected components in colorpred and cnnpred\n",
    "    nb_lbl_color, lbl_color = cv2.connectedComponents(colorpred, connectivity=8)\n",
    "    nb_lbl_cnn, lbl_cnn = cv2.connectedComponents(cnnpred, connectivity=8)\n",
    "\n",
    "    # Create an empty array for the output\n",
    "    output = np.zeros_like(cnnpred)\n",
    "\n",
    "    # Iterate through the connected components in cnnpred\n",
    "    for i in range(1, nb_lbl_cnn):\n",
    "        # Get the mask for the current connected component in cnnpred\n",
    "        cnn_component_mask = (lbl_cnn == i)\n",
    "\n",
    "        # Find all the connected components in colorpred that intersect with the current connected component in cnnpred\n",
    "        color_component_labels = np.unique(lbl_color[cnn_component_mask])\n",
    "        color_component_labels = color_component_labels[color_component_labels != 0]  # Ignore the background component\n",
    "\n",
    "        # Replace the current connected component in cnnpred with the corresponding connected components from colorpred\n",
    "        for color_label in color_component_labels:\n",
    "            output[(lbl_color == color_label)] = 255\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad19b95",
   "metadata": {},
   "source": [
    "#### Color extraction function for this database\n",
    "\n",
    "The hue component is used for colour extraction.  \n",
    "Values are chosen accordingly to the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorextraction(images_list, color_ext):\n",
    "    def extractRColor(src):\n",
    "        \"\"\"Extract the red component in an image and display it\"\"\"\n",
    "\n",
    "        src_hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # # Display the HSV image\n",
    "        # plt.imshow(cv2.cvtColor(src_hsv, cv2.COLOR_HSV2RGB))\n",
    "        # plt.title('HSV Image')\n",
    "        # plt.show()\n",
    "\n",
    "        lower_red1 = np.array([160,40,140])\n",
    "        upper_red1 = np.array([180,255,255])\n",
    "\n",
    "        lower_red2 = np.array([0,40,140]) # empirically determined that 140 was the best threshold for OS\n",
    "        upper_red2 = np.array([12,255,255])\n",
    "\n",
    "        mask = cv2.add(cv2.inRange(src_hsv, lower_red1, upper_red1), \n",
    "                    cv2.inRange(src_hsv, lower_red2, upper_red2))\n",
    "\n",
    "        # # Display the initial mask\n",
    "        # plt.imshow(mask, cmap='gray')\n",
    "        # plt.title('Initial Mask')\n",
    "        # plt.show()\n",
    "\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        kernel_2 = np.ones((2,2), np.uint8)\n",
    "        kernel_hm = np.array((\n",
    "            [0, -1, 0],\n",
    "            [-1, 1, -1],\n",
    "            [0, -1, 0]), dtype=\"int\")\n",
    "\n",
    "        kernel_hm_diag = np.array((\n",
    "            [-1, 0, -1],\n",
    "            [0, 1, 0],\n",
    "            [-1, 0, -1]), dtype=\"int\")\n",
    "\n",
    "        res_hm = cv2.morphologyEx(mask, cv2.MORPH_HITMISS, kernel_hm)\n",
    "        res_hm_diag = cv2.morphologyEx(mask, cv2.MORPH_HITMISS, kernel_hm_diag)\n",
    "        \n",
    "        # # Display hit-miss results\n",
    "        # plt.imshow(res_hm, cmap='gray')\n",
    "        # plt.title('Hit-miss result (kernel_hm)')\n",
    "        # plt.show()\n",
    "\n",
    "        # plt.imshow(res_hm_diag, cmap='gray')\n",
    "        # plt.title('Hit-miss result (kernel_hm_diag)')\n",
    "        # plt.show()\n",
    "\n",
    "        mask = cv2.subtract(mask, res_hm)\n",
    "        mask = cv2.subtract(mask, res_hm_diag)\n",
    "\n",
    "        # # Display mask after hit-miss subtraction\n",
    "        # plt.imshow(mask, cmap='gray')\n",
    "        # plt.title('Mask after hit-miss subtraction')\n",
    "\n",
    "        res_hm = cv2.morphologyEx(mask, cv2.MORPH_HITMISS, kernel_hm)\n",
    "        res_hm_diag = cv2.morphologyEx(mask, cv2.MORPH_HITMISS, kernel_hm_diag)\n",
    "        \n",
    "        # # Display second hit-miss results\n",
    "        # plt.imshow(res_hm, cmap='gray')\n",
    "        # plt.title('Second hit-miss result (kernel_hm)')\n",
    "        # plt.show()\n",
    "\n",
    "        # plt.imshow(res_hm_diag, cmap='gray')\n",
    "        # plt.title('Second hit-miss result (kernel_hm_diag)')\n",
    "        # plt.show()\n",
    "\n",
    "        mask = cv2.subtract(mask, res_hm)\n",
    "        mask = cv2.subtract(mask, res_hm_diag)\n",
    "\n",
    "        # # Display mask after second hit-miss subtraction\n",
    "        # plt.imshow(mask, cmap='gray')\n",
    "        # plt.title('Mask after second hit-miss subtraction')\n",
    "        # plt.show()\n",
    "\n",
    "        lower_white = np.array([253,253,253])\n",
    "        upper_white = np.array([255,255,255])\n",
    "        src_bg = cv2.inRange(src, lower_white, upper_white)\n",
    "        \n",
    "        # # Display white background mask\n",
    "        # plt.imshow(src_bg, cmap='gray')\n",
    "        # plt.title('White background mask')\n",
    "        # plt.show()\n",
    "\n",
    "        src_final = 255 - cv2.add(255 - mask, src_bg)\n",
    "\n",
    "        # # Display the final result\n",
    "        # plt.imshow(src_final, cmap='gray')\n",
    "        # plt.title('Final result')\n",
    "        # plt.show()\n",
    "\n",
    "        return src_final\n",
    "\n",
    "    for filename in tqdm(images_list, total=len(images_list), position=0, leave=True):\n",
    "        filename_wo_path = filename.split('/')[-1].split('.')[0]\n",
    "        large_image = cv2.imread(filename)\n",
    "        red = extractRColor(large_image)\n",
    "        out_path = os.path.join(color_ext, filename_wo_path +'.png')\n",
    "        cv2.imwrite(out_path, red)\n",
    "\n",
    "    return 'Colour extraction completed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c8735",
   "metadata": {},
   "source": [
    "# PATCHIFY CODE START HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0b92a",
   "metadata": {},
   "source": [
    "## Read Image File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c043f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273b699",
   "metadata": {},
   "source": [
    "Modify the path to the model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92220873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 13:08:58.517325: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2023-04-14 13:08:58.584331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:0b:00.0 name: Quadro RTX 5000 computeCapability: 7.5\n",
      "coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.74GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2023-04-14 13:08:58.584432: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n",
      "2023-04-14 13:08:58.589515: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-14 13:08:58.594200: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-14 13:08:58.595415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-14 13:08:58.600999: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-14 13:08:58.603759: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-04-14 13:08:58.615387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-14 13:08:58.616736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-04-14 13:08:58.659419: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2397400000 Hz\n",
      "2023-04-14 13:08:58.673386: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29e65f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-14 13:08:58.673551: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-04-14 13:08:59.087984: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x50d1d60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-14 13:08:59.088237: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 5000, Compute Capability 7.5\n",
      "2023-04-14 13:08:59.090871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:0b:00.0 name: Quadro RTX 5000 computeCapability: 7.5\n",
      "coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 15.74GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2023-04-14 13:08:59.091107: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n",
      "2023-04-14 13:08:59.091450: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-14 13:08:59.091595: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-14 13:08:59.091724: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-14 13:08:59.091861: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-14 13:08:59.091994: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-04-14 13:08:59.092130: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-14 13:08:59.093602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-04-14 13:08:59.093804: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n",
      "2023-04-14 13:09:01.748418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-04-14 13:09:01.748582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2023-04-14 13:09:01.748605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2023-04-14 13:09:01.751011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14902 MB memory) -> physical GPU (device: 0, name: Quadro RTX 5000, pci bus id: 0000:0b:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "#model = tf.keras.models.load_model('/home/alexis/workspace/notebook/models/model_1.h5')\n",
    "model = tf.keras.models.load_model('/home/alexis/workspace/notebook/models/model_1_cus.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15d8788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# MODIFY the path to the image database accordingly\n",
    "base = r'/home/alexis/workspace/DATA/OS/MISSING_FILES'\n",
    "base_classif = r'/home/alexis/workspace/DATA/OS/MISSING_FILES/PREDS'\n",
    "images_list = glob.glob(os.path.join(base, '*.jpg'))\n",
    "images_list.sort()\n",
    "#images_list = images_list[76:]\n",
    "print(len(images_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a1dffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexis/workspace/DATA/OS/MISSING_FILES/Ordnance_Survey_Drawings_-_Christchurch_Bay_(OSD_75-1).jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 13:09:45.125082: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-14 13:09:50.924111: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 333ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:23<00:23, 23.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexis/workspace/DATA/OS/MISSING_FILES/Ordnance_Survey_Drawings_-_Longnor_(OSD_350).jpg\n",
      "8/8 [==============================] - 4s 494ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "for filename in tqdm(images_list, total=len(images_list), position=0, leave=False):\n",
    "    filename_wo_path = filename.split('/')[-1].split('.')[0]\n",
    "    print(filename)\n",
    "    Large_image = cv2.imread(filename, 1)\n",
    "    Large_image = cv2.cvtColor(Large_image, cv2.COLOR_RGB2BGR) \n",
    "    Large_Image=np.array(Large_image)\n",
    "    \n",
    "    img_shape = Large_Image.shape\n",
    "\n",
    "    ## Create Container to hold Image for proper step size\n",
    "    remainderW =  (Large_Image.shape[0] - 256) % 256\n",
    "    remainderH =  (Large_Image.shape[1] - 256) % 256\n",
    "\n",
    "    if remainderW != 0:\n",
    "        width= Large_Image.shape[0] -remainderW +256\n",
    "    else:\n",
    "        width = Large_Image.shape[0]\n",
    "\n",
    "    if remainderH != 0:\n",
    "        height= Large_Image.shape[1] -remainderH +256\n",
    "    else:\n",
    "        height = Large_Image.shape[1]\n",
    "\n",
    "    \n",
    "    container = np.zeros((width, height,3), dtype=int)\n",
    "    cont_shape = container.shape\n",
    "    container[0:Large_Image.shape[0], 0:Large_Image.shape[1],:] = Large_Image[0:Large_Image.shape[0],0:Large_Image.shape[1],:]\n",
    "    #plt.imshow(container)\n",
    "\n",
    "    del Large_Image\n",
    "    \n",
    "    ## Create Patches\n",
    "    patches= patchify(container, (256 ,256,3),step=256)\n",
    "    patches.shape\n",
    "    patches1=patches.reshape(patches.shape[0]*patches.shape[1]*patches.shape[2], 256,256, 3)\n",
    "    patches1.shape\n",
    "\n",
    "    ## Perform Prediction on Patches\n",
    "    #test_pred = model.predict(patches1, verbose=1)\n",
    "\n",
    "    #if memory problems separate the base\n",
    "    maxSize_batch = 1000\n",
    "    n = patches1.shape[0] // maxSize_batch\n",
    "    m = patches1.shape[0] % maxSize_batch\n",
    "    \n",
    "    # if n > 0:\n",
    "    #     test_pred = model.predict(patches1[:maxSize_batch], verbose=1)\n",
    "\n",
    "    #     for i in range(1, n):\n",
    "    #         test_temp = model.predict(patches1[i*maxSize_batch:(i+1)*maxSize_batch], verbose=1)\n",
    "    #         test_pred = np.concatenate((test_pred, test_temp))\n",
    "\n",
    "    #     test_temp = model.predict(patches1[n*maxSize_batch:], verbose=1)\n",
    "    #     test_pred = np.concatenate((test_pred, test_temp))\n",
    "    # else:\n",
    "    #     test_pred = model.predict(patches1[:m], verbose=1)\n",
    "\n",
    "    max_batch_size = 1000\n",
    "    num_batches = (patches1.shape[0] + max_batch_size - 1) // max_batch_size\n",
    "    test_pred = np.empty((0, 256, 256, 1))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * max_batch_size\n",
    "        end_idx = min((i + 1) * max_batch_size, patches1.shape[0])\n",
    "        batch_patches = patches1[start_idx:end_idx]\n",
    "        batch_pred = model.predict(batch_patches, verbose=1)\n",
    "        batch_pred_threshold = batch_pred > 0.7\n",
    "        batch_pred_threshold = batch_pred_threshold.astype(int)\n",
    "        batch_pred_threshold = batch_pred_threshold.reshape((batch_patches.shape[0], 256, 256, 1))\n",
    "        test_pred = np.concatenate((test_pred, batch_pred_threshold))\n",
    "\n",
    "    test_pred_threshold = test_pred > 0.7\n",
    "\n",
    "    ## Consensus\n",
    "    test_pred_threshold = test_pred_threshold.astype(int)\n",
    "    test_pred_threshold = test_pred_threshold.reshape((patches1.shape[0], 256, 256, 1))\n",
    "\n",
    "    del container\n",
    "    \n",
    "    ## Merge Pacthes\n",
    "    #test_pred_threshold=final_pred\n",
    "    #print(test_pred_threshold.shape , patches1.shape)\n",
    "    merge_patches= patches1.reshape(patches.shape[0],patches.shape[1],patches.shape[2], 256,256, 3)\n",
    "    merge_mask = test_pred_threshold.reshape(patches.shape[0],patches.shape[1],patches.shape[2], 256,256, 1)\n",
    "    merge_patches=unpatchify(merge_patches,cont_shape)\n",
    "    merge_masks=unpatchify(merge_mask,(cont_shape[0],cont_shape[1],1))\n",
    "    merge_masks = merge_masks.astype('uint8')\n",
    "    merge_masks[merge_masks==1]=255\n",
    "    merge_masks = merge_masks[0:img_shape[0], 0:img_shape[1],:]\n",
    "\n",
    "    merge_masks[merge_masks>40]=255\n",
    "    merge_masks[merge_masks<40]=0\n",
    "    \n",
    "    del merge_patches\n",
    "    del merge_mask\n",
    "\n",
    "    im_path = os.path.join(base_classif, filename_wo_path +'.tif')\n",
    "    cv2.imwrite(im_path,merge_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e7da91",
   "metadata": {},
   "source": [
    "## Color extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef18722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Colour extraction completed'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODIFY the path to the image database accordingly\n",
    "base = r'/home/alexis/workspace/DATA/OS/MISSING_FILES/'\n",
    "colour_ext = r'/home/alexis/workspace/DATA/OS/MISSING_FILES/COLOR_PREDS'\n",
    "\n",
    "images_list = glob.glob(os.path.join(base, '*.jpg'))\n",
    "images_list.sort()\n",
    "\n",
    "colorextraction(images_list, colour_ext)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03ad47",
   "metadata": {},
   "source": [
    "# Fusion of color extraction and cnn prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff30c4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 images have predictions and colour extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/alexis/workspace/DATA/OS/MISSING_FILES/PREDS/Ordnance_Survey_Drawings_-_Christchurch_Bay_(OSD_75-1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [05:56<11:53, 356.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file: /home/alexis/workspace/DATA/OS/MISSING_FILES/FUSION/Ordnance_Survey_Drawings_-_Christchurch_Bay_(OSD_75-1).tif\n",
      "Processing file: /home/alexis/workspace/DATA/OS/MISSING_FILES/PREDS/Ordnance_Survey_Drawings_-_Longnor_(OSD_350)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [07:35<03:25, 205.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file: /home/alexis/workspace/DATA/OS/MISSING_FILES/FUSION/Ordnance_Survey_Drawings_-_Longnor_(OSD_350).tif\n",
      "Processing file: /home/alexis/workspace/DATA/OS/MISSING_FILES/PREDS/Ordnance_Survey_Drawings_-_Luton_(OSD_148)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [10:35<00:00, 211.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file: /home/alexis/workspace/DATA/OS/MISSING_FILES/FUSION/Ordnance_Survey_Drawings_-_Luton_(OSD_148).tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "#Define paths\n",
    "# MODIFY the path to the image database accordingly\n",
    "base_preds = r'/home/alexis/workspace/DATA/OS/MISSING_FILES/PREDS'\n",
    "color_ext = r'/home/alexis/workspace/DATA/OS/MISSING_FILES/COLOR_PREDS'\n",
    "fusion_ext = r'/home/alexis/workspace/DATA/OS/MISSING_FILES/FUSION'\n",
    "\n",
    "# Get the list of image files\n",
    "images_list = glob.glob(os.path.join(base_preds, '*).tif'))\n",
    "images_list.sort()\n",
    "\n",
    "new_image_list = []\n",
    "\n",
    "# Filter image list by checking the existence of classification files\n",
    "for file_path in images_list:\n",
    "    colorpred_file = get_color_classification_path(file_path, color_ext) + \".png\"\n",
    "    cnnpred_file = get_base_classification_path(file_path, base_preds) + \".tif\"\n",
    "\n",
    "    if classification_files_exist(cnnpred_file, colorpred_file):\n",
    "        new_image_list.append(get_base_classification_path(file_path, base_preds))\n",
    "\n",
    "print(len(new_image_list), \"images have predictions and colour extraction\")\n",
    "\n",
    "# Processing\n",
    "for base_pred_file in tqdm(new_image_list, total=len(new_image_list), position=0, leave=True):\n",
    "    try:\n",
    "        # extract filename from base_pred_file wihtout path and extension\n",
    "        base_fusion_file = get_fusion_path(base_pred_file, fusion_ext)\n",
    "        base_color_file = get_color_classification_path(base_pred_file, color_ext)\n",
    "        print(\"Processing file: \" + base_pred_file)\n",
    "\n",
    "        #if the file has already been processed, skip it\n",
    "        #if os.path.exists(base_fusion_file + '.tif'):\n",
    "            #continue\n",
    "\n",
    "        cnnpred = Image.open(base_pred_file + '.tif')            \n",
    "        cnnpred = cnnpred.convert('L')\n",
    "        cnnpred = np.array(cnnpred)\n",
    "        colorpred = cv2.imread(base_color_file + '.png',0)\n",
    "        src_final = process_images(cnnpred, colorpred)\n",
    "        \n",
    "        # Saving\n",
    "        base_pred_file = os.path.basename(base_pred_file)\n",
    "        print(\"Saving file: \" + base_fusion_file + '.tif')\n",
    "        cv2.imwrite(base_fusion_file + '.tif', src_final)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error processing file: \" + base_pred_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79edbb19",
   "metadata": {},
   "source": [
    "# Detect lines in image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4571fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Set environment variable to access opencv static libraries\n",
    "cv_lib_dir = '../opencv4/build/install_folder/lib:'\n",
    "if os.getenv('LD_LIBRARY_PATH') == None:\n",
    "    os.environ['LD_LIBRARY_PATH'] = cv_lib_dir\n",
    "else:\n",
    "    os.environ['LD_LIBRARY_PATH'] = cv_lib_dir + os.getenv('LD_LIBRARY_PATH')\n",
    "\n",
    "print(os.getenv('LD_LIBRARY_PATH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0096e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classif = 'Image/OS_SURVEY_CLASSIF'\n",
    "images_list = glob.glob(os.path.join(base_classif, '*_fusion_1_pred.png'))\n",
    "images_list.sort()\n",
    "# images_list = images_list[388:]\n",
    "print(len(images_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rep_flash = \"yfaula/yfaula_app/build/\"\n",
    "base_results_flash = \"yfaula/yfaula_app/images/\"\n",
    "\n",
    "compteur = 0\n",
    "\n",
    "for filename in tqdm(images_list, total=len(images_list), position=0, leave=True):\n",
    "    rproc = subprocess.run([\"./\"+ base_rep_flash + \"flash\", \"-p=\"+base_rep_flash + \"flash_default_parameters.txt\", \n",
    "                            \"--output_dir=\" + base_results_flash, filename], capture_output=True)\n",
    "    if rproc.returncode == 0:\n",
    "        compteur += 1\n",
    "    else:\n",
    "        print( 'Problem with : ', filename)\n",
    "        print(rproc.stderr)\n",
    "        \n",
    "print('NB processed images: ', compteur, '/', len(images_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105373f",
   "metadata": {},
   "source": [
    "## We use the same image to generate the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results_flash = \"Image/os_survey_flashlines/\"\n",
    "base_classif = 'Image/OS_SURVEY_CLASSIF/'\n",
    "images_list = glob.glob(os.path.join(base_classif, '*_fusion_1_pred.png'))\n",
    "images_list.sort()\n",
    "#images_list = images_list[388:]\n",
    "print(len(images_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa38fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compteur=0\n",
    "\n",
    "for filename in tqdm(images_list, total=len(images_list), position=0, leave=True):\n",
    "    im_flash_path = base_results_flash + filename.split('/')[-1].split('.')[0] + \"_flashlines.png\"\n",
    "    \n",
    "    pred = cv2.imread(filename, 0)\n",
    "    \n",
    "    if os.path.exists(im_flash_path):\n",
    "        lines = cv2.imread(im_flash_path, 0)\n",
    "        \n",
    "        ### POST PROCESSING OF LINE DETECTION : 2 possibilities :\n",
    "        ## change the if condition in order to choose a solution (False or True)\n",
    "        ## 1) (SOLTUION PROPOSAL 1) enlarge the detection to the adjacent pixels\n",
    "        if False:\n",
    "            nb_lbl, lbl = cv2.connectedComponents(pred,connectivity=8)\n",
    "\n",
    "            #pred_merge_and  = cv2.bitwise_and(pred, cnnpred)\n",
    "\n",
    "            counter_lbl = [ 0 ] * nb_lbl\n",
    "            counter_lbl_lines = [ 0 ] * nb_lbl\n",
    "\n",
    "            for i in range(lbl.shape[0]):\n",
    "                for j in range(lbl.shape[1]):\n",
    "                    if pred[i, j]:\n",
    "                        counter_lbl[lbl[i, j]] += 1\n",
    "                    if lines[i, j] and pred[i, j]:\n",
    "                        counter_lbl_lines[lbl[i, j]] += 1\n",
    "\n",
    "            for i in range(pred.shape[0]):\n",
    "                for j in range(pred.shape[1]):\n",
    "                    # between .20 and .33 because of flash mask size\n",
    "                    if pred[i, j] > 0 and counter_lbl[lbl[i, j]] > 0:\n",
    "                        pred[i, j] = 0 if counter_lbl_lines[lbl[i, j]]/counter_lbl[lbl[i, j]] > 0.25  else 255\n",
    "\n",
    "            pred_final = pred\n",
    "            \n",
    "        else:\n",
    "        ## 2) (BASIC SOLUTION) enlarge the detection with morpholgy operations\n",
    "            ker = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (6, 6)) # depends on the size of flash kernel (7, 7)\n",
    "            lines = cv2.dilate(lines, ker, iterations=1)\n",
    "            \n",
    "            mask_flash2 = cv2.bitwise_and(lines, pred)\n",
    "            pred_final = cv2.bitwise_xor(pred, mask_flash2)\n",
    "        \n",
    "        compteur+=1\n",
    "    else:\n",
    "        pred_final = pred\n",
    "\n",
    "    cv2.imwrite(base_classif + filename.split('/')[-1].split('.')[0] + '_final.tif', pred_final)\n",
    "\n",
    "print('NB lines elimination: ', compteur)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1f05059",
   "metadata": {},
   "source": [
    "## (OPTIONAL) A try to remove lines with Hough transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ec1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing\n",
    "from queue import Queue\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.chdir('/home/alexis/workspace/BACKUP_HDD/images/OS/OS_SURVEY_CLASSIF')\n",
    "path_out = r'/home/alexis/workspace/BACKUP_HDD/images/OS/LINE_REMOVED/'\n",
    "\n",
    "minLineLength = 10\n",
    "maxLineGap = 100\n",
    "threshold = 500\n",
    "\n",
    "# define a function to detect paralell lines and merge them\n",
    "def merge_lines(img, image_name):\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply a Canny edge detector to the image\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "\n",
    "    # Apply morphological closing operation to fill in gaps between lines\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n",
    "    closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Apply the Hough transform to detect lines\n",
    "    lines = cv2.HoughLinesP(closed, 1, np.pi/180, 100, minLineLength=minLineLength, maxLineGap=maxLineGap)\n",
    "\n",
    "    # Extract the x, y coordinates of the lines\n",
    "    coords = []\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            coords.append([(x1, y1), (x2, y2)])\n",
    "\n",
    "    # Remove invalid lines\n",
    "    coords = [line for line in coords if line is not None]\n",
    "\n",
    "    # Merge lines that are both parallel and intersect\n",
    "    new_coords = []\n",
    "    for i in range(len(coords)):\n",
    "        if coords[i] is None:\n",
    "            continue\n",
    "        for j in range(i + 1, len(coords)):\n",
    "            if coords[j] is None:\n",
    "                continue\n",
    "            line1 = coords[i]\n",
    "            line2 = coords[j]\n",
    "            if line1 is None:\n",
    "                continue           \n",
    "            p1, p2 = line1\n",
    "            p3, p4 = line2\n",
    "            d1 = np.array(p2) - np.array(p1)\n",
    "            d2 = np.array(p4) - np.array(p3)\n",
    "            norm_d1 = np.linalg.norm(d1)\n",
    "            norm_d2 = np.linalg.norm(d2)\n",
    "            if norm_d1 > 0 and norm_d2 > 0:\n",
    "                cos_theta = np.dot(d1, d2) / (norm_d1 * norm_d2)\n",
    "                if abs(cos_theta) > 0.95:  # threshold for parallel lines\n",
    "                    d = np.cross(d1, d2)\n",
    "                    if abs(d) > 1e-10:  # threshold for intersecting lines\n",
    "                        p1 = np.array(p1).ravel()\n",
    "                        p2 = np.array(p2).ravel()\n",
    "                        t1 = np.cross(p3 - p1, d2) / d\n",
    "                        t2 = np.cross(p1 - p3, d1) / (-d)\n",
    "                        if 0 <= t1 <= norm_d2 and 0 <= t2 <= norm_d1:\n",
    "                            intersection = np.array(p1) + t1 * d1 / norm_d1\n",
    "                            new_coords.append([p1, intersection, p2])\n",
    "                            new_coords.append([p3, intersection, p4])\n",
    "                            # Remove processed lines from the list\n",
    "                            coords[i] = None\n",
    "                            coords[j] = None\n",
    "\n",
    "    # Add remaining unprocessed lines to the list\n",
    "    for line in coords:\n",
    "        if line is not None:\n",
    "            new_coords.append(line)\n",
    "\n",
    "    # Draw the merged lines on the image\n",
    "    img_lines = img.copy()\n",
    "    for line in new_coords:\n",
    "        if len(line) == 2:\n",
    "            x1, y1 = line[0]\n",
    "            x2, y2 = line[1]\n",
    "            # filter out lines that are too short (shorter than threshold)\n",
    "            if np.linalg.norm(np.array(line[0]) - np.array(line[1])) > threshold:\n",
    "                cv2.line(img_lines, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 0), 3)\n",
    "        elif len(line) == 3:\n",
    "            x1, y1 = line[0]\n",
    "            x2, y2 = line[2]\n",
    "            # filter out lines that are too short (shorter than threshold)\n",
    "            if np.linalg.norm(np.array(line[0]) - np.array(line[2])) > threshold:\n",
    "                cv2.line(img_lines, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 0), 3)\n",
    "    return(img_lines, image_name)\n",
    "\n",
    "# define a function to detect connected groups of pixels \n",
    "def dilate_img(img):\n",
    "    # convert image to grayscale\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # threshold image to create binary mask\n",
    "    ret, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # find connected components in binary mask\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(thresh, connectivity=8)\n",
    "\n",
    "    # create new mask with only large enough components\n",
    "    new_mask = np.zeros(thresh.shape, dtype=np.uint8)\n",
    "    for i in range(1, num_labels):\n",
    "        if stats[i, cv2.CC_STAT_AREA] >= 150:\n",
    "            new_mask[labels == i] = 255\n",
    "\n",
    "    # dilate the large enough components\n",
    "    kernel = np.ones((75, 75), np.uint8) \n",
    "    dilated_mask = cv2.dilate(new_mask, kernel)\n",
    "\n",
    "    # apply the new mask to the input image\n",
    "    dilated_img = cv2.bitwise_and(img, dilated_mask)\n",
    "    return dilated_img\n",
    "\n",
    "def save_image(img, image_name):\n",
    "    image_name = image_name.replace('_fusion_1_pred_final', '')\n",
    "    image_path_out = os.path.join(path_out, f\"{image_name}_CLEANED.png\")\n",
    "    try:\n",
    "        cv2.imwrite(image_path_out, img)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the image: {e}\")\n",
    "\n",
    "# define a function to detect lines using LSD\n",
    "def lsd_line_detector(img):\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "    # Apply LSD to detect line segments\n",
    "    #lsd = cv2.createLineSegmentDetector(0)\n",
    "    lsd = cv2.ximgproc.createFastLineDetector()\n",
    "    lines = lsd.detect(gray)\n",
    "\n",
    "    # create a mask with the detected lines of the same size as the image\n",
    "    mask = np.zeros_like(img, dtype=np.uint8)\n",
    "    mask = lsd.drawSegments(mask, lines)\n",
    "\n",
    "    # draw the lines on the image\n",
    "    img_lines = img.copy()\n",
    "    img_lines = lsd.drawSegments(img_lines, lines)\n",
    "\n",
    "    # save the image with the detected lines\n",
    "    #image_name = image_name.replace('_fusion_1_pred_final', '')\n",
    "    #image_path_out = os.path.join(path_out, f\"{image_name}_LSD.png\")\n",
    "    #cv2.imwrite(image_path_out, img_lines)\n",
    "\n",
    "    # prolong each lines by 30 pixels in both directions\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = list(map(int, np.ravel(np.asarray(line, dtype=int))))\n",
    "            d = np.array([x2 - x1, y2 - y1])\n",
    "            d = d / np.linalg.norm(d)   \n",
    "            x1 = int(x1 - d[0])\n",
    "            y1 = int(y1 - d[1])\n",
    "            x2 = int(x2 + d[0])\n",
    "            y2 = int(y2 + d[1])\n",
    "            cv2.line(mask, (x1, y1), (x2, y2), (255, 255, 255), 3)\n",
    "\n",
    "    #combine as one element the lines that are close to each other\n",
    "    mask = cv2.dilate(mask, np.ones((5, 5), np.uint8), iterations=5) \n",
    "\n",
    "    gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "    contours, hierarchy = cv2.findContours(gray_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    filtered_contours = []\n",
    "    heights = []\n",
    "    widths = []\n",
    "    dilated_mask = dilate_img(img)\n",
    "\n",
    "    for contour in contours:\n",
    "        # Create a binary mask for the current contour\n",
    "        contour_mask = np.zeros_like(gray_mask)\n",
    "        cv2.drawContours(contour_mask, [contour], -1, 255, -1)\n",
    "\n",
    "        # Apply bitwise AND operation to get the pixels inside the contour\n",
    "        contour_pixels = cv2.bitwise_and(dilated_mask, contour_mask)\n",
    "\n",
    "        # Calculate the number of white pixels in the contour\n",
    "        white_pixels = cv2.countNonZero(contour_pixels)\n",
    "\n",
    "        #Calculate the total number of pixels in the contour\n",
    "        total_pixels = cv2.contourArea(contour)\n",
    "\n",
    "        # Calculate the ratio between the number of white pixels and the area of the contour\n",
    "        ratio = round((white_pixels / total_pixels*100), 2)\n",
    "        \n",
    "        # Calculate the bounding box of the contour\n",
    "        rect = cv2.minAreaRect(contour)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "        w = int(rect[1][0])\n",
    "        h = int(rect[1][1])\n",
    "        heights.append(h)\n",
    "        widths.append(w)\n",
    "        aspect_ratio = round(float(w) / h, 2)\n",
    "\n",
    "        # select contour with lower level of white pixels density:\n",
    "        if w > 200 or h > 200: \n",
    "            if ratio < 14:\n",
    "                #if needed print the ratio on the image next to the shape\n",
    "                #cv2.putText(img, str(ratio), (x+50, y+50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2)\n",
    "                filtered_contours.append(contour)\n",
    "        \n",
    "        elif w > 100 or h > 100:\n",
    "            # then filter out contours that are not thin or wide enough\n",
    "            if aspect_ratio < 0.3 or aspect_ratio > 3:\n",
    "                #cv2.putText(img, str(aspect_ratio), (x+50, y+50), cv2.FONT_HERSHEY_SIMPLEX, 2, (225, 255, 0), 2)\n",
    "                filtered_contours.append(contour)\n",
    "        else:\n",
    "            if w > 250 or h > 250:\n",
    "            # then filter out contours that are not thin or wide enough\n",
    "                aspect_ratio = round(float(w) / h, 2)\n",
    "                if aspect_ratio < 0.8 or aspect_ratio > 1.2:\n",
    "                    #cv2.putText(img, str(aspect_ratio), (x+50, y+50), cv2.FONT_HERSHEY_SIMPLEX, 2, (225, 255, 0), 2)\n",
    "                    filtered_contours.append(contour)\n",
    "                            \n",
    "    # fill the inside of the contours with black pixels\n",
    "    img_filtered = cv2.drawContours(img, filtered_contours, -1, (0, 0, 0), -1)\n",
    "\n",
    "    return img_filtered\n",
    "\n",
    "# LSD line detector #2\n",
    "def lsd_line_detector2(img, img_to_apply, dist_thresh):\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "    # Apply LSD to detect line segments\n",
    "    #lsd = cv2.createLineSegmentDetector(0)\n",
    "    lsd = cv2.ximgproc.createFastLineDetector()\n",
    "    lines = lsd.detect(gray)\n",
    "\n",
    "    # create a mask with the detected lines of the same size as the image\n",
    "    mask = np.zeros_like(img, dtype=np.uint8)\n",
    "    mask = lsd.drawSegments(mask, lines)\n",
    "\n",
    "    # draw the lines on the image\n",
    "    img_lines = img.copy()\n",
    "    img_lines = lsd.drawSegments(img_lines, lines)\n",
    "\n",
    "    # prolong each lines by some pixels in both directions\n",
    "    if lines is not None: \n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = list(map(int, np.ravel(np.asarray(line, dtype=int))))\n",
    "            d = np.array([x2 - x1, y2 - y1])\n",
    "            d = d / np.linalg.norm(d)\n",
    "            d = d * dist_thresh\n",
    "            x1 = int(x1 - d[0])\n",
    "            y1 = int(y1 - d[1])\n",
    "            x2 = int(x2 + d[0])\n",
    "            y2 = int(y2 + d[1])\n",
    "            cv2.line(mask, (x1, y1), (x2, y2), (255, 255, 255), 3)\n",
    "\n",
    "    #combine as one element the lines that are close to each other\n",
    "    mask = cv2.dilate(mask, np.ones((3, 3), np.uint8), iterations=2) \n",
    "    gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "    contours, hierarchy = cv2.findContours(gray_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    filtered_contours = []\n",
    "    heights = []\n",
    "    widths = []\n",
    "\n",
    "    for contour in contours:\n",
    "        # Create a binary mask for the current contour\n",
    "        contour_mask = np.zeros_like(gray_mask)\n",
    "        cv2.drawContours(contour_mask, [contour], -1, 255, -1)\n",
    "\n",
    "        # Calculate the bounding box of the contour\n",
    "        rect = cv2.minAreaRect(contour)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "        w = int(rect[1][0])\n",
    "        h = int(rect[1][1])\n",
    "        heights.append(h)\n",
    "        widths.append(w)\n",
    "        aspect_ratio = round(float(w) / h, 2)\n",
    "\n",
    "        # Filter out contours that are not thin or wide enough\n",
    "        if w > 100 or h > 100:\n",
    "            if aspect_ratio < 0.2 or aspect_ratio > 5:\n",
    "                filtered_contours.append(contour)\n",
    "                            \n",
    "    # fill the inside of the contours with black pixels and draw the contours on the image\n",
    "    img_filtered = cv2.drawContours(img_to_apply, filtered_contours, -1, (0, 0, 0), -1)\n",
    "\n",
    "    return img_filtered\n",
    "\n",
    "def process_image(image_name, queue):\n",
    "    img = cv2.imread(image_name)\n",
    "\n",
    "    # merge parallel lines\n",
    "    img_merged, _ = merge_lines(img, image_name)\n",
    "\n",
    "    # apply LSD line detector\n",
    "    lsd_filtered = lsd_line_detector(img_merged)\n",
    "\n",
    "    # apply LSD line detector #2\n",
    "    lsd_filtered2 = lsd_line_detector2(img, lsd_filtered, 30)\n",
    "\n",
    "    # apply LSD line detector #3\n",
    "    lsd_filtered3 = lsd_line_detector2(lsd_filtered2, lsd_filtered2, 15)\n",
    "\n",
    "    # save image\n",
    "    save_image(lsd_filtered3, image_name)\n",
    "\n",
    "    # add cleaned image to queue\n",
    "    queue.put((lsd_filtered3, image_name))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_dir = r'/home/alexis/workspace/BACKUP_HDD/images/OS/OS_SURVEY_CLASSIF/'\n",
    "\n",
    "    # get list of image files\n",
    "    image_files = [f for f in os.listdir(input_dir) if f.endswith('_fusion_1_pred_final2.tif')]\n",
    "    #image_files = image_files[:5]\n",
    "\n",
    "    # create queue to hold processed images\n",
    "    queue = Queue()\n",
    "\n",
    "    # create list of processes\n",
    "    processes = []\n",
    "    for image_name in image_files:\n",
    "        process = multiprocessing.Process(target=process_image, args=(image_name, queue))\n",
    "        processes.append(process)\n",
    "\n",
    "    # start processes\n",
    "    for process in processes:\n",
    "        process.start()\n",
    "\n",
    "    # wait for processes to finish\n",
    "    for process in tqdm(processes):\n",
    "        process.join()\n",
    "\n",
    "    # retrieve processed images from queue and save to output directory\n",
    "    while not queue.empty():\n",
    "        img, image_name = queue.get()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92108bc2",
   "metadata": {},
   "source": [
    "## CALCULATE ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY the path to the image database accordingly\n",
    "base = r'/home/alexis/workspace/DATA/OS/GT/images/'\n",
    "base_classif = r'/home/alexis/workspace/DATA/OS/GT/preds/'\n",
    "images_list = glob.glob(os.path.join(base, '*.tif'))\n",
    "images_list.sort()\n",
    "print(len(images_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e62196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict images from OS GT dataset\n",
    "for filename in tqdm(images_list, total=len(images_list), position=0, leave=False):\n",
    "    filename_wo_path = filename.split('/')[-1].split('.')[0]\n",
    "    print(filename)\n",
    "    Large_image = cv2.imread(filename, 1)\n",
    "    Large_image = cv2.cvtColor(Large_image, cv2.COLOR_RGB2BGR) \n",
    "    Large_Image=np.array(Large_image)\n",
    "    \n",
    "    img_shape = Large_Image.shape\n",
    "\n",
    "    ## Create Container to hold Image for proper step size\n",
    "    remainderW =  (Large_Image.shape[0] - 256) % 256\n",
    "    remainderH =  (Large_Image.shape[1] - 256) % 256\n",
    "\n",
    "    if remainderW != 0:\n",
    "        width= Large_Image.shape[0] -remainderW +256\n",
    "    else:\n",
    "        width = Large_Image.shape[0]\n",
    "\n",
    "    if remainderH != 0:\n",
    "        height= Large_Image.shape[1] -remainderH +256\n",
    "    else:\n",
    "        height = Large_Image.shape[1]\n",
    "\n",
    "    \n",
    "    container = np.zeros((width, height,3), dtype=int)\n",
    "    cont_shape = container.shape\n",
    "    container[0:Large_Image.shape[0], 0:Large_Image.shape[1],:] = Large_Image[0:Large_Image.shape[0],0:Large_Image.shape[1],:]\n",
    "    #plt.imshow(container)\n",
    "\n",
    "    del Large_Image\n",
    "    \n",
    "    ## Create Patches\n",
    "    patches= patchify(container, (256 ,256,3),step=256)\n",
    "    patches.shape\n",
    "    patches1=patches.reshape(patches.shape[0]*patches.shape[1]*patches.shape[2], 256,256, 3)\n",
    "    patches1.shape\n",
    "\n",
    "    ## Perform Prediction on Patches\n",
    "    #test_pred = model.predict(patches1, verbose=1)\n",
    "\n",
    "    #si probleme mémoire séparer la base\n",
    "    maxSize_batch = 1000\n",
    "    n = patches1.shape[0] // maxSize_batch\n",
    "    m = patches1.shape[0] % maxSize_batch\n",
    "    \n",
    "    max_batch_size = 1000\n",
    "    num_batches = (patches1.shape[0] + max_batch_size - 1) // max_batch_size\n",
    "    test_pred = np.empty((0, 256, 256, 1))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * max_batch_size\n",
    "        end_idx = min((i + 1) * max_batch_size, patches1.shape[0])\n",
    "        batch_patches = patches1[start_idx:end_idx]\n",
    "        batch_pred = model.predict(batch_patches, verbose=1)\n",
    "        batch_pred_threshold = batch_pred > 0.7\n",
    "        batch_pred_threshold = batch_pred_threshold.astype(int)\n",
    "        batch_pred_threshold = batch_pred_threshold.reshape((batch_patches.shape[0], 256, 256, 1))\n",
    "        test_pred = np.concatenate((test_pred, batch_pred_threshold))\n",
    "\n",
    "\n",
    "    test_pred_threshold = test_pred > 0.7\n",
    "\n",
    "    ## Consensus\n",
    "    test_pred_threshold = test_pred_threshold.astype(int)\n",
    "    test_pred_threshold = test_pred_threshold.reshape((patches1.shape[0], 256, 256, 1))\n",
    "\n",
    "    del container\n",
    "    \n",
    "    ## Merge Pacthes\n",
    "    #test_pred_threshold=final_pred\n",
    "    #print(test_pred_threshold.shape , patches1.shape)\n",
    "    merge_patches= patches1.reshape(patches.shape[0],patches.shape[1],patches.shape[2], 256,256, 3)\n",
    "    merge_mask = test_pred_threshold.reshape(patches.shape[0],patches.shape[1],patches.shape[2], 256,256, 1)\n",
    "    merge_patches=unpatchify(merge_patches,cont_shape)\n",
    "    merge_masks=unpatchify(merge_mask,(cont_shape[0],cont_shape[1],1))\n",
    "    merge_masks = merge_masks.astype('uint8')\n",
    "    merge_masks[merge_masks==1]=255\n",
    "    merge_masks = merge_masks[0:img_shape[0], 0:img_shape[1],:]\n",
    "\n",
    "    merge_masks[merge_masks>40]=255\n",
    "    merge_masks[merge_masks<40]=0\n",
    "    \n",
    "    del merge_patches\n",
    "    del merge_mask\n",
    "\n",
    "\n",
    "    im_path = os.path.join(base_classif, filename_wo_path + '.png')\n",
    "    cv2.imwrite(im_path,merge_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_into_patches(image, patch_size):\n",
    "    patches = []\n",
    "    h, w = image.shape\n",
    "    for i in range(0, h, patch_size):\n",
    "        for j in range(0, w, patch_size):\n",
    "            patch = image[i:i+patch_size, j:j+patch_size]\n",
    "            patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "def merge_patches_into_image(patches, original_shape):\n",
    "    h, w = original_shape\n",
    "    patch_size = patches[0].shape[0]\n",
    "    rows = (h + patch_size - 1) // patch_size\n",
    "    cols = (w + patch_size - 1) // patch_size\n",
    "    merged_image = np.zeros(original_shape, dtype=np.uint8)\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(0, rows):\n",
    "        for j in range(0, cols):\n",
    "            patch = patches[idx]\n",
    "            y1, y2 = i * patch_size, (i + 1) * patch_size\n",
    "            x1, x2 = j * patch_size, (j + 1) * patch_size\n",
    "            y2 = min(y2, h)\n",
    "            x2 = min(x2, w)\n",
    "            merged_image[y1:y2, x1:x2] = patch[:y2 - y1, :x2 - x1]\n",
    "            idx += 1\n",
    "\n",
    "    return merged_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede86a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from patchify import patchify, unpatchify\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "base_preds = r'/home/alexis/workspace/DATA/OS/GT/preds/'\n",
    "color_ext = r'/home/alexis/workspace/DATA/OS/GT/color/'\n",
    "fusion_ext = r'/home/alexis/workspace/DATA/OS/GT/fusion/'\n",
    "\n",
    "colorextraction(images_list, color_ext)\n",
    "\n",
    "new_image_list = []\n",
    "\n",
    "# Filter image list by checking the existence of classification files\n",
    "for file_path in images_list:\n",
    "    colorpred_file = get_color_classification_path(file_path, color_ext) + \".png\"\n",
    "    cnnpred_file = get_base_classification_path(file_path, base_preds) + \".png\"\n",
    "    print(cnnpred_file, colorpred_file)\n",
    "\n",
    "    if classification_files_exist(cnnpred_file, colorpred_file):\n",
    "        new_image_list.append(get_base_classification_path(file_path, base_preds))\n",
    "\n",
    "print(len(new_image_list), \"images have predictions and colour extraction\")\n",
    "\n",
    "# Processing\n",
    "patch_size = 256\n",
    "\n",
    "for base_pred_file in tqdm(new_image_list, total=len(new_image_list), position=0, leave=True):\n",
    "    try:\n",
    "        # extract filename from base_pred_file without path and extension\n",
    "        base_fusion_file = get_fusion_path(base_pred_file, fusion_ext)\n",
    "        base_color_file = get_color_classification_path(base_pred_file, color_ext)\n",
    "        cnnpred = Image.open(base_pred_file + '.png')\n",
    "        # convert to 8-bit format\n",
    "        cnnpred = cnnpred.convert('L')\n",
    "        cnnpred = np.array(cnnpred)\n",
    "        colorpred = cv2.imread(base_color_file + '.tif', 0)\n",
    "\n",
    "        # Split images into smaller patches\n",
    "        cnnpred_patches = split_image_into_patches(cnnpred, patch_size)\n",
    "        colorpred_patches = split_image_into_patches(colorpred, patch_size)\n",
    "\n",
    "        # Apply process_images function on each patch\n",
    "        processed_patches = []\n",
    "        for i in tqdm(range(len(cnnpred_patches))):\n",
    "            processed_patch = process_images(cnnpred_patches[i], colorpred_patches[i])\n",
    "            processed_patches.append(processed_patch)\n",
    "\n",
    "        # Merge processed patches\n",
    "        src_final = merge_patches_into_image(processed_patches, cnnpred.shape)\n",
    "\n",
    "        # Saving\n",
    "        base_pred_file = os.path.basename(base_pred_file)\n",
    "        cv2.imwrite(base_fusion_file + '.tif', src_final)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error processing file: \" + base_pred_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the fusion files are flipped use\n",
    "#flip horzontally and rotate 180 degrees the fusion images\n",
    "for file in tqdm(os.listdir(fusion_ext), total=len(os.listdir(fusion_ext)), position=0, leave=True):\n",
    "    try:\n",
    "        img = cv2.imread(os.path.join(fusion_ext, file), 0)\n",
    "        img = cv2.flip(img, 1)\n",
    "        img = cv2.rotate(img, cv2.ROTATE_180)\n",
    "        cv2.imwrite(os.path.join(fusion_ext, file), img)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error processing file: \" + file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# load the predicted masks\n",
    "preds = '/home/alexis/workspace/DATA/OS/GT/fusion/'\n",
    "preds_list = []\n",
    "for img in os.listdir(preds):\n",
    "    if img.endswith(\".tif\"):\n",
    "        preds_list.append(img)\n",
    "\n",
    "# load the ground truth masks\n",
    "gt_folder = '/home/alexis/workspace/DATA/OS/GT/labels/'\n",
    "gt_list = []\n",
    "for img in os.listdir(gt_folder):\n",
    "    if img.endswith(\".tif\"):\n",
    "        gt_list.append(img)\n",
    "\n",
    "# only keep the images that are in both folders\n",
    "preds_list = [x for x in preds_list if x in gt_list]\n",
    "gt_list = [x for x in gt_list if x in preds_list]\n",
    "\n",
    "print('Number of predicted masks: ', len(preds_list))\n",
    "print('Number of ground truth masks: ', len(gt_list))\n",
    "\n",
    "# create dictionaries to store the precision, recall, F1 score and IoU, with image names as keys\n",
    "precision_dict = {}\n",
    "recall_dict = {}\n",
    "f1_score_dict = {}\n",
    "iou_dict = {}\n",
    "\n",
    "for i in range(len(preds_list)):\n",
    "    pred = Image.open(os.path.join(preds,preds_list[i]))\n",
    "    gt = Image.open(os.path.join(gt_folder,gt_list[i]))\n",
    "    \n",
    "    # convert to 8-bit format\n",
    "    pred = pred.convert('L')\n",
    "    gt = gt.convert('L')\n",
    "    \n",
    "    # convert to numpy arrays\n",
    "    pred = np.array(pred)\n",
    "    gt = np.array(gt)\n",
    "    \n",
    "    # in the gt mask all values above 0 are considered as 1\n",
    "    gt[gt > 0] = 1\n",
    "    pred[pred > 0] = 1\n",
    "\n",
    "    # compute precision, recall, and F1 score, for 0 and 1 values\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(gt.ravel(), pred.ravel())\n",
    "    precision_dict[preds_list[i]] = precision\n",
    "    recall_dict[preds_list[i]] = recall\n",
    "    f1_score_dict[preds_list[i]] = f1_score\n",
    "\n",
    "    # compute IoU\n",
    "    IoU = jaccard_score(gt.ravel(), pred.ravel(), average=None)\n",
    "    iou_dict[preds_list[i]] = IoU\n",
    "\n",
    "# print the mean precision, recall, IoU, and F1 score\n",
    "print('Mean precision: ', np.mean(list(precision_dict.values())))\n",
    "print('Mean recall: ', np.mean(list(recall_dict.values())))\n",
    "print('Mean IoU: ', np.mean(list(iou_dict.values())))\n",
    "print('Mean F1 score: ', np.mean(list(f1_score_dict.values())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
